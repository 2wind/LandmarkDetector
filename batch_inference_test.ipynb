{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0781a2a4cf53b5641473fad8676e8c0a38872e1ed5a7fb83d02846a3135ad21c8",
   "display_name": "Python 3.8.8 64-bit ('outline': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "781a2a4cf53b5641473fad8676e8c0a38872e1ed5a7fb83d02846a3135ad21c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     C:\\Users\\yongwook\\anaconda3\n",
      "outline               *  C:\\Users\\yongwook\\anaconda3\\envs\\outline\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "!conda info --env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "source": [
    "## Network\n",
    "Based on https://colab.research.google.com/drive/1-28T5nIAevrDo6MwN0Qi_Cgdy9TEiSP_?usp=sharing#scrollTo=XH_bqPXo6YG8\n",
    "\n",
    "Resnext50을 이용한다. 일단은 Greyscale(컬러로 확장도 가능하나 실익이 크지 않다.)\n",
    "\n",
    "https://towardsdatascience.com/face-landmarks-detection-with-pytorch-4b4852f5e9c4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,num_classes=(landmark_number*2)):\n",
    "        super().__init__()\n",
    "        self.model_name='resnet50'\n",
    "        #self.model=models.resnet18(pretrained=True)\n",
    "        \n",
    "        self.model=models.resnet50(pretrained=True)\n",
    "\n",
    "\n",
    "        self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # for param in self.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        # RGB:        self.model.conv1=nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        #self.model.conv1=nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        #self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.model.fc=nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_distance(landmark, reference):\n",
    "    ''' pixel_distance(landmark: np.array[[x, y], ..] \n",
    "                        reference: np.array[[x, y]] with true landmark value\n",
    "\n",
    "        return: average: float average distance,\n",
    "                each: np.array[distance, ..] with distance of each landmark\n",
    "    '''\n",
    "    each = []\n",
    "    for i in range(len(landmark)):\n",
    "        each.append(np.linalg.norm(landmark[i] - reference[i]))\n",
    "\n",
    "    each = np.array(each)\n",
    "    average = np.average(each)\n",
    "\n",
    "    return average, each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_difference(landmark, reference):\n",
    "    each = np.empty((len(landmark), 2))\n",
    "    for i in range(len(landmark)):\n",
    "        each[i] = (landmark[i] - reference[i])\n",
    "\n",
    "    print(each)\n",
    "    \n",
    "    return each"
   ]
  },
  {
   "source": [
    "## From Face detection to landmark detection, IRL\n",
    "\n",
    "https://github.com/timesler/facenet-pytorch\n",
    "\n",
    "- With pip:\n",
    "pip install facenet-pytorch\n",
    "\n",
    "- or clone this repo, removing the '-' to allow python imports:\n",
    "git clone https://github.com/timesler/facenet-pytorch.git facenet_pytorch\n",
    "\n",
    "- or use a docker container (see https://github.com/timesler/docker-jupyter-dl-gpu):\n",
    "docker run -it --rm timesler/jupyter-dl-gpu pip install facenet-pytorch && ipython"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "def return_path (common_path):\n",
    "    # common_path = r'AutoAlign_test\\B25776___________000_lat'\n",
    "    pi, pt, fi, ft = \"_photo.jpg\", \"_photo.txt\", \"_film.jpg\", \"_film.txt\"\n",
    "    image_path = common_path + pi\n",
    "    tsv_path = common_path + pt\n",
    "    film_path = common_path + ft\n",
    "    film_img_path = common_path + fi\n",
    "\n",
    "    return image_path, tsv_path, film_img_path, film_path\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_resize(im, desired_size=1024):\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "\n",
    "    # thumbnail is a in-place operation\n",
    "\n",
    "    # im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    # create a new image and paste the resized on it\n",
    "\n",
    "    new_im = Image.new(\"RGB\", (desired_size, desired_size), (255,255,255))\n",
    "    new_im.paste(im, (0,0))\n",
    "\n",
    "    return new_im\n",
    "\n",
    "def load_tsv(path):\n",
    "    # Loading dataframe\n",
    "    df = pd.read_csv(path,  sep='\\t')\n",
    "    df = df.iloc[:99, 0:3]\n",
    "    \n",
    "    df.columns = ['name', 'X', 'Y']\n",
    "\n",
    "    return df\n",
    "\n",
    "def extract_landmarks(df, landmark_regex, landmark_length):\n",
    "    # (gathering only needed landmarks)\n",
    "    df = df.loc[df['name'].str.contains(landmark_regex, regex=True), :]\n",
    "    # there are **18** landmarks that is unique and valid among all files\n",
    "    # should we sort df?\n",
    "    df = df.sort_values(by=['name'])\n",
    "    df = df.loc[:, ['X', 'Y']]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # ... and landmark\n",
    "    landmark = df.to_numpy(dtype=np.float32)\n",
    "    return landmark\n",
    "\n",
    "def transform_landmarks(matrix, landmarks):\n",
    "    '''\n",
    "    transform_landmarks(matrix: np.array(), landmarks: np.array()):\n",
    "        matrix: numpy.array(), a 2x3 matrix array which is affine transformation.\n",
    "        landmarks: numpy.array(), a (n, 2) shaped array that contains landmarks information.\n",
    "\n",
    "    1. Add \"1\" to each coordinate. (x, y) --> (x, y, 1)\n",
    "    2. Transpose and multiply with matrix.\n",
    "        [[a, b, c],   [[x,         [[x',\n",
    "         [d, e, f]] X   y,      =    y'], ...]\n",
    "                        1], ...] \n",
    "            matrix  X landmarks = transformed matrix\n",
    "    3. Transpose back to original format, and return.\n",
    "\n",
    "    returns:\n",
    "        result: numpy.array(), a (n, 2) shaped array that contains transformed landmarks information.\n",
    "        \n",
    "    '''\n",
    "    ones = np.ones((1, len(landmarks)))\n",
    "    homography_landmarks = np.concatenate((landmarks, ones.T), axis=1)\n",
    "    result = np.dot(matrix, homography_landmarks.T).T\n",
    "    return result\n",
    "\n",
    "def find_original_scale(size, original_info):\n",
    "    width, height = original_info\n",
    "    new_width, new_height = size, size\n",
    "    if width >= height:\n",
    "        new_height = height / width * size\n",
    "    else:\n",
    "        new_width = width / height * size\n",
    "\n",
    "    return new_width, new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input images: 50\n",
      "C:\\Users\\yongwook\\anaconda3\\envs\\outline\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "C:\\Users\\yongwook\\anaconda3\\envs\\outline\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  boxes = np.array(boxes)\n",
      "C:\\Users\\yongwook\\anaconda3\\envs\\outline\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  points = np.array(points)\n",
      "face not detected: B19358___________000_lat\n",
      "face detected: 49\n",
      "======== landmark avg ==========\n",
      "40374___________000_lat average: [0.02205823 0.01780756] | False\n",
      "B4867___________000_lat average: [0.00303025 0.00416456] | True\n",
      "B7524___________000_lat average: [0.02662856 0.0100958 ] | False\n",
      "B13616___________000_lat average: [0.00187316 0.00451644] | True\n",
      "25160___________000_lat average: [0.01285987 0.01752303] | False\n",
      "B25611___________000_lat average: [0.0057532  0.00487935] | True\n",
      "B4151___________000_lat average: [0.0014622  0.00776098] | True\n",
      "19951816___________000_lat average: [0.00501336 0.01719048] | False\n",
      "48301___________000_lat average: [0.00286184 0.01379597] | True\n",
      "B16939___________000_lat average: [0.00191867 0.00531133] | True\n",
      "B17545___________000_lat average: [0.00467457 0.01364503] | True\n",
      "B5080___________000_lat average: [0.00269714 0.0039181 ] | True\n",
      "40275___________000_lat average: [0.0046332  0.01091929] | True\n",
      "0101___________000_lat average: [0.00425019 0.00703008] | True\n",
      "B10243___________000_lat average: [0.0061216  0.00640712] | True\n",
      "B15809___________000_lat average: [0.0025367  0.02791131] | False\n",
      "17285___________000_lat average: [0.04091021 0.00424684] | False\n",
      "12488___________000_lat average: [0.00278348 0.01425031] | True\n",
      "B25776___________000_lat average: [0.00325491 0.00678067] | True\n",
      "14644377___________000_lat average: [0.00756692 0.02113192] | False\n",
      "B22163___________000_lat average: [0.00467552 0.01658864] | False\n",
      "B15167___________000_lat average: [0.00260076 0.00273453] | True\n",
      "22248198___________000_lat average: [0.0012059  0.00680095] | True\n",
      "B12007___________000_lat average: [0.00321069 0.00657656] | True\n",
      "B15955___________000_lat average: [0.0118075  0.00582047] | True\n",
      "01042185___________3618_lat average: [0.0149306  0.00653697] | False\n",
      "B8753___________000_lat average: [0.00226388 0.00498494] | True\n",
      "48329___________000_lat average: [0.00774168 0.02330757] | False\n",
      "B17915___________000_lat average: [0.00085167 0.00432014] | True\n",
      "B23418___________000_lat average: [0.00724352 0.0097288 ] | True\n",
      "B19336___________000_lat average: [0.00449061 0.01395094] | True\n",
      "B13247___________000_lat average: [0.00521943 0.00843936] | True\n",
      "B7887___________000_lat average: [0.00180497 0.00593824] | True\n",
      "B22050___________000_lat average: [0.01391509 0.00559688] | True\n",
      "23987___________000_lat average: [0.01157329 0.01429793] | False\n",
      "B19857___________000_lat average: [0.00209469 0.00837647] | True\n",
      "45325___________000_lat average: [0.01735263 0.02179659] | False\n",
      "B10606___________000_lat average: [0.02146823 0.00853607] | False\n",
      "B19011___________000_lat average: [0.00218925 0.00848639] | True\n",
      "B4667___________000_lat average: [0.00258276 0.01244914] | True\n",
      "B9871___________000_lat average: [0.00385686 0.02205997] | False\n",
      "B6863___________000_lat average: [0.00178203 0.00936662] | True\n",
      "B11375___________000_lat average: [0.00216883 0.00968555] | True\n",
      "B12007___________001_lat average: [0.00237599 0.00551919] | True\n",
      "B14138___________000_lat average: [0.00238633 0.00944273] | True\n",
      "B23013___________000_lat average: [0.00189567 0.01008623] | True\n",
      "28800___________000_lat average: [0.00516196 0.0088384 ] | True\n",
      "43476___________000_lat average: [0.00729403 0.01510303] | False\n",
      "21706747___________000_lat average: [0.02931459 0.01032053] | False\n",
      "==========transform avg==========\n",
      "40374___________000_lat average: [0.09238671 0.0574266 ] | False\n",
      "B4867___________000_lat average: [0.00461129 0.00743798] | True\n",
      "B7524___________000_lat average: [0.04011999 0.04589229] | False\n",
      "B13616___________000_lat average: [0.00540947 0.0068553 ] | True\n",
      "25160___________000_lat average: [0.10920575 0.0798158 ] | False\n",
      "B25611___________000_lat average: [0.01252174 0.01685999] | False\n",
      "B4151___________000_lat average: [0.01420887 0.02697009] | False\n",
      "19951816___________000_lat average: [0.0774762  0.06217404] | False\n",
      "48301___________000_lat average: [0.04775491 0.03239786] | False\n",
      "B16939___________000_lat average: [0.01268607 0.02396295] | False\n",
      "B17545___________000_lat average: [0.0196003  0.02198842] | False\n",
      "B5080___________000_lat average: [0.00778023 0.01200639] | True\n",
      "40275___________000_lat average: [0.05911728 0.04320737] | False\n",
      "0101___________000_lat average: [0.03852962 0.02661107] | False\n",
      "B10243___________000_lat average: [0.01062369 0.01338645] | False\n",
      "B15809___________000_lat average: [0.0351183  0.06643782] | False\n",
      "17285___________000_lat average: [0.03086282 0.01184018] | False\n",
      "12488___________000_lat average: [0.05279076 0.0385482 ] | False\n",
      "B25776___________000_lat average: [0.01583201 0.02380824] | False\n",
      "14644377___________000_lat average: [0.08003329 0.04776534] | False\n",
      "B22163___________000_lat average: [0.00793523 0.01067695] | True\n",
      "B15167___________000_lat average: [0.0078317  0.01208138] | True\n",
      "22248198___________000_lat average: [0.01654112 0.0090277 ] | False\n",
      "B12007___________000_lat average: [0.01309112 0.0175752 ] | False\n",
      "B15955___________000_lat average: [0.01558892 0.01958619] | False\n",
      "01042185___________3618_lat average: [0.04562541 0.03647301] | False\n",
      "B8753___________000_lat average: [0.0053669  0.00600972] | True\n",
      "48329___________000_lat average: [0.11933296 0.08713779] | False\n",
      "B17915___________000_lat average: [0.00180268 0.00368311] | True\n",
      "B23418___________000_lat average: [0.01079773 0.01654937] | False\n",
      "B19336___________000_lat average: [0.02590129 0.04547559] | False\n",
      "B13247___________000_lat average: [0.01937908 0.03451934] | False\n",
      "B7887___________000_lat average: [0.0123361 0.0219739] | False\n",
      "B22050___________000_lat average: [0.01896269 0.02131739] | False\n",
      "23987___________000_lat average: [0.05826477 0.04258429] | False\n",
      "B19857___________000_lat average: [0.01528152 0.02891003] | False\n",
      "45325___________000_lat average: [0.03262779 0.02935228] | False\n",
      "B10606___________000_lat average: [0.01985814 0.02354214] | False\n",
      "B19011___________000_lat average: [0.00399878 0.00950178] | True\n",
      "B4667___________000_lat average: [0.02669188 0.05049646] | False\n",
      "B9871___________000_lat average: [0.01917828 0.03246752] | False\n",
      "B6863___________000_lat average: [0.00644027 0.01409617] | False\n",
      "B11375___________000_lat average: [0.01351623 0.0255704 ] | False\n",
      "B12007___________001_lat average: [0.02135715 0.03804281] | False\n",
      "B14138___________000_lat average: [0.01875096 0.03465399] | False\n",
      "B23013___________000_lat average: [0.00799244 0.01509707] | False\n",
      "28800___________000_lat average: [0.04559512 0.03329389] | False\n",
      "43476___________000_lat average: [0.0432302  0.02804127] | False\n",
      "21706747___________000_lat average: [0.06241199 0.04327603] | False\n",
      "landmark average distance under 1%: 33\n",
      "landmark average distance under 3%: 49\n",
      "film image corner distance under 1%: 8\n",
      "film image corner distance under 3%: 30\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "\n",
    "valid_prefix = ['40374___________000_lat', 'B4867___________000_lat', 'B7524___________000_lat', 'B13616___________000_lat', '25160___________000_lat', 'B25611___________000_lat', 'B4151___________000_lat', '19951816___________000_lat', '48301___________000_lat', 'B16939___________000_lat', 'B17545___________000_lat', 'B5080___________000_lat', '40275___________000_lat', '0101___________000_lat', 'B10243___________000_lat', 'B15809___________000_lat', '17285___________000_lat', '12488___________000_lat', 'B25776___________000_lat', '14644377___________000_lat', 'B22163___________000_lat', 'B15167___________000_lat', '22248198___________000_lat', 'B12007___________000_lat', 'B15955___________000_lat', '01042185___________3618_lat', 'B8753___________000_lat', '48329___________000_lat', 'B17915___________000_lat', 'B23418___________000_lat', 'B19358___________000_lat', 'B19336___________000_lat', 'B13247___________000_lat', 'B7887___________000_lat', 'B22050___________000_lat', '23987___________000_lat', 'B19857___________000_lat', '45325___________000_lat', 'B10606___________000_lat', 'B19011___________000_lat', 'B4667___________000_lat', 'B9871___________000_lat', 'B6863___________000_lat', 'B11375___________000_lat', 'B12007___________001_lat', 'B14138___________000_lat', 'B23013___________000_lat', '28800___________000_lat', '43476___________000_lat', '21706747___________000_lat']\n",
    "\n",
    "# If required, create a face detection pipeline using MTCNN:\n",
    "mtcnn = MTCNN(device=device)\n",
    "\n",
    "best_network = Network()\n",
    "# best_network.load_state_dict(torch.load(f\"model/{time_str}_{landmark_number}_{num_epochs}_{weights_path}_{best_network.model_name}.tar\")['network_state_dict'])\n",
    "best_network.load_state_dict(torch.load(f\"model/0602_1303_6_100_face_landmarks_transfer__resnet50.tar\")['network_state_dict'])\n",
    "best_network.eval()\n",
    "\n",
    "names = []\n",
    "photo_images = []\n",
    "photo_landmarks = []\n",
    "film_landmarks = []\n",
    "original_info = []\n",
    "film_sizes = []\n",
    "\n",
    "root = \"AutoAlign_test/\"\n",
    "for name in valid_prefix:\n",
    "    image_path, tsv_path, film_img_path, film_path = return_path(root + name)\n",
    "    names.append(name)\n",
    "\n",
    "    input_image = Image.open(image_path)\n",
    "    original_info.append(input_image.size)\n",
    "    input_image = padded_resize(input_image, 320)\n",
    "    photo_images.append(input_image)\n",
    "\n",
    "    photo_landmark = extract_landmarks(load_tsv(tsv_path), '29@[2479]|30@[34]', 6)\n",
    "    film_landmark = extract_landmarks(load_tsv(film_path), '29@[2479]|30@[34]', 6)\n",
    "    photo_landmarks.append(photo_landmark)\n",
    "    film_landmarks.append(film_landmark)\n",
    "\n",
    "    film_sizes.append(Image.open(film_img_path).size)\n",
    "\n",
    "\n",
    "print(f\"input images: {len(photo_images)}\")\n",
    "\n",
    "# grayscale_image = input_image.convert('L')\n",
    "# height, width = input_image.size[0], input_image.size[1]\n",
    "# print(height, width)\n",
    "# Get cropped and prewhitened image tensor\n",
    "boxes, probs = mtcnn.detect(photo_images)\n",
    "# print(boxes)\n",
    "\n",
    "crop_images = []\n",
    "crop_info = []\n",
    "\n",
    "for count, box in enumerate(boxes):\n",
    "    image = photo_images[count]\n",
    "    grayscale_image = image.convert('L')\n",
    "\n",
    "    # print(box)\n",
    "    if box is None:\n",
    "        print(f\"face not detected: {names[count]}\")\n",
    "        names[count] = None\n",
    "        photo_images[count] = None\n",
    "        photo_landmarks[count] = None\n",
    "        film_landmarks[count] = None\n",
    "        original_info[count] = None\n",
    "        film_sizes[count] = None\n",
    "\n",
    "    if (box is not None):\n",
    "        face = box[0]\n",
    "        x0, y0, x1, y1 = face\n",
    "        face_width = x1 - x0\n",
    "        x0, y0, x1, y1 = int(x0)+face_width * 0.05, int(y0), int(x1)+face_width * 0.05, int(y1) \n",
    "        crop_image = TF.resized_crop(grayscale_image, y0, x0, y1-y0, x1-x0, size=(224, 224))\n",
    "        crop_image = TF.to_tensor(crop_image)\n",
    "        crop_image = TF.normalize(crop_image, [0.6945], [0.33497])\n",
    "        crop_images.append(crop_image)\n",
    "        crop_info.append((x0, y0, x1, y1))\n",
    "    # print(count, box, image)\n",
    "\n",
    "names = [x for x in names if x is not None]\n",
    "photo_images = [x for x in photo_images if x is not None]\n",
    "photo_landmarks = [x for x in photo_landmarks if x is not None]\n",
    "film_landmarks = [x for x in film_landmarks if x is not None]\n",
    "original_info = [x for x in original_info if x is not None]\n",
    "film_sizes = [x for x in film_sizes if x is not None]\n",
    "\n",
    "print(f\"face detected: {len(crop_images)}\")\n",
    "\n",
    "batch_input = torch.stack(crop_images)\n",
    "with torch.no_grad():\n",
    "   landmarks = best_network(batch_input)\n",
    "\n",
    "result = []\n",
    "for count, info in enumerate(crop_info):\n",
    "    landmark = landmarks[count].view(landmark_number,2).detach().numpy() + 0.5\n",
    "    landmark = (landmark * np.array([[info[2]-info[0], info[3]-info[1]]]) + np.array([[info[0], info[1]]]))\n",
    "    result.append(landmark)\n",
    "\n",
    "# Uncomment to show image of landmark on photo\n",
    "\n",
    "# figure, axis = plt.subplots(10, 5)\n",
    "# plt.figure(figsize=(30,15))\n",
    "# for count, image in enumerate(photo_images):\n",
    "#     landmark = result[count]\n",
    "#     axis[count % 10, count // 10].imshow(image)\n",
    "#     axis[count % 10, count // 10].scatter(landmark[:,0], landmark[:,1], c = 'c', s = 1)\n",
    "# plt.show()\n",
    "\n",
    "dist, rect = [], []\n",
    "for count, (name, info, photo_landmark, landmark_found, film_landmark, film_size) in enumerate(zip(names, original_info, photo_landmarks, result, film_landmarks, film_sizes)):\n",
    "    \n",
    "    fixed_landmark = landmark_found * np.array(info) / np.array(find_original_scale(320, info))\n",
    "    landmark_dist = np.average(np.abs(fixed_landmark-photo_landmark) / np.array(info), axis=0)\n",
    "    dist.append(landmark_dist)\n",
    "\n",
    "    matrix, _ = cv2.estimateAffinePartial2D(film_landmark, fixed_landmark, method=cv2.LMEDS)\n",
    "    sol_matrix, _ = cv2.estimateAffinePartial2D(film_landmark, photo_landmark, method=cv2.LMEDS)\n",
    "    corners = np.array([[0,0], [0, film_size[1]], [film_size[0], 0], [film_size[0], film_size[1]]])\n",
    "    \n",
    "    ours = transform_landmarks(matrix, corners)\n",
    "    sol = transform_landmarks(sol_matrix, corners)\n",
    "\n",
    "    avr = np.average(np.abs(ours-sol) / np.array(info), axis=0)\n",
    "    rect.append(avr)\n",
    "\n",
    "dist_count = 0\n",
    "dist_count_loose = 0\n",
    "rect_count = 0    \n",
    "rect_count_loose = 0    \n",
    "print(\"======== landmark avg ==========\")\n",
    "for i, d in enumerate(dist):\n",
    "    print(f\"{names[i]} average: {d} | {np.average(d) < 0.01}\")\n",
    "    if np.average(d) < 0.01:\n",
    "        dist_count += 1\n",
    "    if np.average(d) < 0.03:\n",
    "        dist_count_loose += 1\n",
    "\n",
    "print(\"==========transform avg==========\")\n",
    "for i, avr in enumerate(rect):\n",
    "    print(f\"{names[i]} average: {avr} | {np.average(avr) < 0.01}\")\n",
    "    if np.average(avr) < 0.01:\n",
    "        rect_count += 1\n",
    "    if np.average(avr) < 0.03:\n",
    "        rect_count_loose += 1\n",
    "    \n",
    "print(f\"landmark average distance under 1%: {dist_count}\")\n",
    "print(f\"landmark average distance under 3%: {dist_count_loose}\")\n",
    "print(f\"film image corner distance under 1%: {rect_count}\")\n",
    "print(f\"film image corner distance under 3%: {rect_count_loose}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}